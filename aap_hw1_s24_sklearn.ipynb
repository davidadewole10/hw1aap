{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc17827",
   "metadata": {},
   "source": [
    "# HW1 - Classification models in sklearn\n",
    "\n",
    "You'll be building a few classifier models and using some of the tech tools we learned about in Modules 1 and 2. \n",
    "\n",
    "## The Raw Data\n",
    "\n",
    "The data is the the KC housing data. **I've made all the necessary data files available to you\n",
    "in the assignment folder.**\n",
    "\n",
    "Kaggle source: https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "Basic data dictionary\n",
    "\n",
    "https://geodacenter.github.io/data-and-lab//KingCounty-HouseSales2015/\n",
    "\n",
    "Link to discussion item meaning of CONDITION and GRADE fields:\n",
    "\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction/discussion/141767\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2bda9-61ef-45ea-a67b-4bedba8f63be",
   "metadata": {},
   "source": [
    "## Preliminary Data Prep\n",
    "In order to use this data for a classification problem, I did some data prep work. Our target variable is a new variable that I created called `price_gt_1M` which is a binary variable:\n",
    "\n",
    "* 1 - house price is greater than or equal to 1 million dollars\n",
    "* 0 - house price is less than a million dollars\n",
    "\n",
    "The data for this classification problem can be found in `./data/kc_house_data_classification.csv`.\n",
    "\n",
    "If you want to see my data prep code, see the `hw1_sklearn_dataprep.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a3b77-cd10-454e-bb65-cebbfb51ae3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Problem\n",
    "\n",
    "Our overall goal is to build classifier models to predict `price_gt_1M` using the the other variables. You must use sklearn Pipelines that contain your preprocessing steps and your model estimation step. We did this in the class notes.\n",
    "\n",
    "You should do your work in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78147e4-e1c0-4175-ab38-c05b52514037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Task 1 - Folder structure\n",
    "\n",
    "Start by creating a new project folder structure with the `cookiecutter-datascience-simple` template that I covered in Module 1. Put the data files into an appropriate folder and put this notebook in the main project folder. Any additional notebooks and/or Python files you end up creating should go in the main project folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ea0e1-f002-4cc3-a8b3-847872f89d01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 2 - Version control\n",
    "\n",
    "Put your new project folder under version control using git. You should **NOT** track the data file. You must track all notebooks, Python scripts or additional text files you end up creating. Put appropriate information into your readme file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef43a51-cc77-4fb0-863e-ec3f4a74a646",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3 - EDA\n",
    "\n",
    "I suggest you start by reading the csv file into a pandas dataframe. I called my dataframe, `housing_df`.\n",
    "Then start with some basic EDA. You can certainly use automated tools such as pandas-profiling, skimpy or SweetViz as I showed in the class notes. Remember, when you run some of those tools, you **must** have your notebook open in the classic Jupyter Notebook interface (and **NOT** in Jupyter Lab) Check their docs to see if Jupyter Lab is supported yet. I pip installed SweetViz and it seems to be working fine now with Jupyter Lab. As we've seen, the reports get created as HTML documents. These should go in your output folder within your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e608898a-d4b6-42e6-84ea-ee3e5a00c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea4c7d9-866f-46d5-a9bd-f514a7e8b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Cleaned Classification File\n",
    "housing_df = pd.read_csv(\"./data/kc_house_data_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fecf1148-4701-403f-a0ed-fd35d1186b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "0      20141013T000000  221900.0         3       1.00         1180      5650   \n",
       "1      20141209T000000  538000.0         3       2.25         2570      7242   \n",
       "2      20150225T000000  180000.0         2       1.00          770     10000   \n",
       "3      20141209T000000  604000.0         4       3.00         1960      5000   \n",
       "4      20150218T000000  510000.0         3       2.00         1680      8080   \n",
       "...                ...       ...       ...        ...          ...       ...   \n",
       "21608  20140521T000000  360000.0         3       2.50         1530      1131   \n",
       "21609  20150223T000000  400000.0         4       2.50         2310      5813   \n",
       "21610  20140623T000000  402101.0         2       0.75         1020      1350   \n",
       "21611  20150116T000000  400000.0         3       2.50         1600      2388   \n",
       "21612  20141015T000000  325000.0         2       0.75         1020      1076   \n",
       "\n",
       "       floors  waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "0         1.0           0     0          3      7        1180              0   \n",
       "1         2.0           0     0          3      7        2170            400   \n",
       "2         1.0           0     0          3      6         770              0   \n",
       "3         1.0           0     0          5      7        1050            910   \n",
       "4         1.0           0     0          3      8        1680              0   \n",
       "...       ...         ...   ...        ...    ...         ...            ...   \n",
       "21608     3.0           0     0          3      8        1530              0   \n",
       "21609     2.0           0     0          3      8        2310              0   \n",
       "21610     2.0           0     0          3      7        1020              0   \n",
       "21611     2.0           0     0          3      8        1600              0   \n",
       "21612     2.0           0     0          3      7        1020              0   \n",
       "\n",
       "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \n",
       "0          1955             0    98178  47.5112 -122.257           1340  \n",
       "1          1951          1991    98125  47.7210 -122.319           1690  \n",
       "2          1933             0    98028  47.7379 -122.233           2720  \n",
       "3          1965             0    98136  47.5208 -122.393           1360  \n",
       "4          1987             0    98074  47.6168 -122.045           1800  \n",
       "...         ...           ...      ...      ...      ...            ...  \n",
       "21608      2009             0    98103  47.6993 -122.346           1530  \n",
       "21609      2014             0    98146  47.5107 -122.362           1830  \n",
       "21610      2009             0    98144  47.5944 -122.299           1020  \n",
       "21611      2004             0    98027  47.5345 -122.069           1410  \n",
       "21612      2008             0    98144  47.5941 -122.299           1020  \n",
       "\n",
       "[21613 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview Dataframe\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e48e24a-96c0-4986-9b04-97b7caae5d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           21613 non-null  object \n",
      " 1   price          21613 non-null  float64\n",
      " 2   bedrooms       21613 non-null  int64  \n",
      " 3   bathrooms      21613 non-null  float64\n",
      " 4   sqft_living    21613 non-null  int64  \n",
      " 5   sqft_lot       21613 non-null  int64  \n",
      " 6   floors         21613 non-null  float64\n",
      " 7   waterfront     21613 non-null  int64  \n",
      " 8   view           21613 non-null  int64  \n",
      " 9   condition      21613 non-null  int64  \n",
      " 10  grade          21613 non-null  int64  \n",
      " 11  sqft_above     21613 non-null  int64  \n",
      " 12  sqft_basement  21613 non-null  int64  \n",
      " 13  yr_built       21613 non-null  int64  \n",
      " 14  yr_renovated   21613 non-null  int64  \n",
      " 15  zipcode        21613 non-null  int64  \n",
      " 16  lat            21613 non-null  float64\n",
      " 17  long           21613 non-null  float64\n",
      " 18  sqft_living15  21613 non-null  int64  \n",
      "dtypes: float64(5), int64(13), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis\n",
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63b1429-cb0a-4041-9bf7-6b01de82745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sweetviz\n",
      "  Downloading sweetviz-2.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from sweetviz) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from sweetviz) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.1.3 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from sweetviz) (3.8.4)\n",
      "Collecting tqdm>=4.43.0 (from sweetviz)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.6 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 41.0/57.6 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 378.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from sweetviz) (1.13.0)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from sweetviz) (3.1.3)\n",
      "Collecting importlib-resources>=1.2.0 (from sweetviz)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from jinja2>=2.11.1->sweetviz) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from tqdm>=4.43.0->sweetviz) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\david\\anaconda3\\envs\\aap\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n",
      "Downloading sweetviz-2.3.1-py3-none-any.whl (15.1 MB)\n",
      "   ---------------------------------------- 0.0/15.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/15.1 MB 4.8 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.3/15.1 MB 2.9 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/15.1 MB 3.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.7/15.1 MB 3.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.0/15.1 MB 4.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.2/15.1 MB 4.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.4/15.1 MB 4.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/15.1 MB 4.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.8/15.1 MB 4.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.9/15.1 MB 4.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.1/15.1 MB 4.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.3/15.1 MB 4.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.5/15.1 MB 4.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.7/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.9/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.1/15.1 MB 4.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.3/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.5/15.1 MB 4.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.9/15.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.9/15.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.3/15.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.5/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.7/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.0/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.2/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.4/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.6/15.1 MB 4.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.8/15.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.1/15.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.2/15.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.5/15.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.7/15.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.9/15.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.0/15.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.2/15.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.3/15.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.4/15.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.6/15.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.8/15.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.0/15.1 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.1/15.1 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.2/15.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.4/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.6/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.8/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.9/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.1/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.2/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.4/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.5/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.6/15.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.8/15.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.9/15.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.1/15.1 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.4/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.6/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.8/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.0/15.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.2/15.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.4/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.6/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.7/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.0/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.2/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.4/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.6/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.8/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.0/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.3/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.4/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.6/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.8/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.0/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.1/15.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.3/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.6/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/15.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.0/15.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.1/15.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.1/15.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.1/15.1 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 61.4/78.3 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, importlib-resources, sweetviz\n",
      "Successfully installed importlib-resources-6.4.0 sweetviz-2.3.1 tqdm-4.66.4\n"
     ]
    }
   ],
   "source": [
    "#Automated EDA with SweetViz\n",
    "!pip install sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d46b5bce-cfaa-46bf-a942-b193ec960150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |█████████████████████████████████████████| [100%]   00:01 -> (00:00 left)\n"
     ]
    }
   ],
   "source": [
    "# Import and create sweetviz report - Automated EDA (See report in Output Folder).\n",
    "import sweetviz\n",
    "report = sweetviz.analyze(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201d62a3-8d9e-4a51-9ecb-b45d828d755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report output/sweetviz_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "# Save Report in html format in the output folder\n",
    "report.show_html(\"output/sweetviz_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebd035-5afa-48e0-8a7e-e25b7bbe28b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4 - Categorize feature types\n",
    "\n",
    "As we did in the Pump it Up class notes, we are going to need to create a list of categorical variables and a list of numeric variables so that we\n",
    "can apply the appropriate pre-processing to each. In the notes we used the data type of the columns to create lists of numeric and categorical variables. That's not necessarily going to work here as all the variables will come in as numeric. So, you'll have to come up with another way to create lists of the categorical variables and the numeric variables. \n",
    "\n",
    "Since we are using regularization, all of the numeric variables will need to rescaled using the `StandardScaler`. You'll do this later as part of the `Pipeline`. For any variables that you decide should be treated as categorical in your models, use the `OneHotEncoder` on them in the preprocessing stage.\n",
    "\n",
    "Be careful, just because a variable has a numeric datatype in the pandas dataframe, it does **not** mean that it's necessarily a numeric variable in the context of the classification models. Think about each column and look at your EDA reports and decide whether or not it's truly numeric or needs to be treated as categorical data in the models.  \n",
    "\n",
    "Even though our target variable, `price_gt_1M`, is categorical, you do **NOT** need to do any preprocessing on it. As I mentioned in our class notes, scikit-learn will automatically detect that and will do any encoding needed on its own.\n",
    "\n",
    "Finally, you'll partition the dataset into training and test datasets for modeling: \n",
    "\n",
    "* I broke up the `housing_df` into two separate dataframes that I called `X` and `y`, to use in the models. Here's my code for that:\n",
    "\n",
    "```\n",
    "X = housing_df.iloc[:, 0:18]\n",
    "y = housing_df.iloc[:, 18]\n",
    "```\n",
    "\n",
    "* Please use the following code for your data partitioning so that we all end up with the same training and test split:\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb25f3-5980-4ddd-9c60-baba45ea6449",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4 - Logistic regression models\n",
    "\n",
    "You are going to build a few different logistic regression models using all of the variables in our housing dataset. For each of these models you will:\n",
    "\n",
    "- Create a pipeline to do the preprocessing (the scaling and encoding) and the modeling (we did this in the Pump it Up project)\n",
    "- I'll be giving you different specifications and hyperparameter parameter settings to try\n",
    "- You'll be scoring the models on overall accuracy for both the training and test data. Discuss any evidence of overfitting or underfitting as well as how the model does in comparison to the null model.\n",
    "- There will be some additional tasks/questions for each model - details below\n",
    "\n",
    "**IMPORTANT** You always should put summary comments in one or more markdown cells. Do **NOT** write them as comments in a code cell. The whole point of Jupyter notebooks is to be able to mix markdown cells with code cells. Yes, you should also include code comments but those are different than analysis comments.\n",
    "\n",
    "#### Model 0: The null model\n",
    "\n",
    "We always start with the simplest possible model and we call it the *null model*. For binary classification models, the null model is usually just to predict that each observation will fall into whichever class is most prevalent. In other words, what would be the performance of a model in which we just predict 0 for everyone? Scikit-learn has a way to create simple null models for classification with the `sklearn.dummy.DummyClassifier` class. See https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html for the details. You must use this class to create your null model.\n",
    "\n",
    "#### Model 1: Ridge regression with C=1.0\n",
    "\n",
    "Build a ridge regression model to predict `price_gt_1M` and use the default value of `C=1.0`. I used the following additional options with the `LogisticRegression` model - `solver='saga', max_iter=2000`. Feel free to change these if you want. AFter fitting the model, compute its accuracy score for training and test and write out a little summary (f-strings are useful). Here's an example:\n",
    "\n",
    "    Training score: 0.974\n",
    "    Test score: 0.971\n",
    "\n",
    "Create confusion matrices for both training and test.\n",
    "\n",
    "Also, create a plot of the coefficients (as we did in the notes). If you want to use that `coef_plot` function we used in the notes, you'll have\n",
    "to make a few modifications because we only have one set of coefficients (since we have a binary classification problem as opposed to a 3-class problem in Pump it Up).\n",
    "\n",
    "#### Model 2: Lasso regression with C=1.0\n",
    "\n",
    "Same as Model 1, but use lasso regression instead of ridge regression. Create the same outputs and compare the performance to the ridge regression model.\n",
    "\n",
    "#### Model 3: Lasso regression with C=0.01\n",
    "\n",
    "Fit another lasso regression but use `C=0.01`. Does this enforce more or less regularization? Create the same outputs and compare the performance to the first two models. Discuss why the plot looks so different than the previous plots.\n",
    "\n",
    "#### Model 4: Lasso regression with optimal C value\n",
    "\n",
    "Now use `LogisticRegressionCV` to fit a model and let sklearn determine the optimal `C` value to use. Again, compute the accuracy score and confusion matrices. Also, print out the optimal value of `C`. Does regularization help for this problem?\n",
    "\n",
    "### Task 5 - Simple decision tree\n",
    "Now fit a decision tree to predict `price_gt_1M`. As we did above, for both train and test, compute the accuracy score, create a confusion matrix, and discuss the performance relative to your logistic regression models. Obviously you do not need to create a coefficient plot (why not?). \n",
    "\n",
    "**HACKER EXTRA:** See if you can figure out how to display the fitted tree so that it's readable. :)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70793e38-4f88-4777-82a7-7db804c1a77d",
   "metadata": {},
   "source": [
    "### Task 6 - a little error exploration (more challenging)\n",
    "\n",
    "This will challenge your pandas and your general data manipulation skills. Just give it your best shot. If you don't get, you don't get it. It doesn't require much code - just the right code. :)\n",
    "\n",
    "I also include another data file called `kc_house_data_regression.csv` in which the target variable is `price`. Everything else is exactly the same, including the order of the rows. So, here's your challenge. Using Model 2 (the lasso model with `C=1.0`), start by using the `predict` method to generate an array of predictions for the original test data. Obviously, some of the predictions are correct and some of them are not. It would be interesting to know more about the kinds of errors our model is making. We can see some things from the confusion matrix. However, since we don't have the actual `price` value, it's hard to visualize how the errors relate to it. For example, are we only making errors when the price is really close to 1 million? One way to visualize this is to create a histogram of the actual prices **only for those rows in test that we got wrong**. What makes this tricky? A few things:\n",
    "\n",
    "* As I already mentioned, `price` is not in our original data but is in the `kc_house_data_regression.csv` dataset. Remember, other than the target variable, this dataset is identical (including the index) to the one we used above for classification.\n",
    "* We partitioned the classification dataset into training and test datasets.\n",
    "* In order to create the histogram, we simply need a Series (or array) of `price` values corresponding to the predictions in test that we got wrong.\n",
    "\n",
    "**HINTS** \n",
    "\n",
    "* The pandas `join` method will come in handy.\n",
    "* The pandas `.loc` selector can take a boolean array as an input for selecting rows or columns. Using one to select rows is quite useful for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026c076",
   "metadata": {},
   "source": [
    "## Optional Hacker Extra Credit tasks\n",
    "I always like to include some extra credit tasks for those who want to push themselves a little further. For this problem, consider doing one or more of the following:\n",
    "\n",
    "* Try out the [Histogram based Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html?highlight=histogram%20based%20gradient%20boosting%20classifier) shown in the optional materials at the end of Module 2. Compare its performance to logistic regression and the random forest.\n",
    "* I also include another data file called `kc_house_data_regression.csv` in which the target variable is `price`. Use sklearn's `LassoCV` to find a good regression model for predicting `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32e527",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "You should simply compress your entire project folder as either a zip file or a tar.gz file (do **NOT** ever use WinRAR to create rar files in this class). Note that when you do this, your \"hidden\" `.git` folder will get included. So, I'll be able to tell that you put the project under version control and I'll be able to look at your project folder structure. Before compressing the project folder to submit it:\n",
    "\n",
    "* make sure all of your notebooks and other files are in the main project folder and have good filenames,\n",
    "* make sure you've committed all of your changes (git),\n",
    "* upload your compressed folder in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
